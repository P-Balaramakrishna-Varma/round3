{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7d096ffd-c4ce-4d79-b0a6-ca711701c6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from torch import nn\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c96d0602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Data Size:  30000\n",
      "Online Data Size:  30000\n",
      "Test Data Size:  10000\n"
     ]
    }
   ],
   "source": [
    "## Dataloading both intial and online data\n",
    "class SplitDataset(Dataset):\n",
    "    def __init__(self, start, end, transform):\n",
    "        self.train_dataset = torchvision.datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.end - self.start\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.train_dataset[idx + self.start]\n",
    "\n",
    "## Traning Data\n",
    "# transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,),)])\n",
    "transform = transforms.ToTensor()\n",
    "train_dataset = torchvision.datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "\n",
    "## Initial and Online Data splitting\n",
    "split = len(train_dataset) // 2\n",
    "last_idx = len(train_dataset)\n",
    "initial_data = SplitDataset(0, split, transform)\n",
    "online_data = SplitDataset(split, last_idx, transform)\n",
    "\n",
    "## Intial and online dataloader\n",
    "initial_dataloader = DataLoader(initial_data, batch_size=32, shuffle=True)\n",
    "online_dataloader = DataLoader(initial_data, batch_size=32, shuffle=True)\n",
    "\n",
    "\n",
    "## Test Data\n",
    "testset = torchvision.datasets.MNIST('/tmp', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=32, shuffle=False)\n",
    "\n",
    "print(\"Initial Data Size: \", len(initial_data))\n",
    "print(\"Online Data Size: \", len(online_data))\n",
    "print(\"Test Data Size: \", len(testset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "40949384-bac2-47f6-9410-e5e907abae65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "## Creating the model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9f6e8659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 36.0%, Avg loss: 2.260007 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 2.199481 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 59.9%, Avg loss: 2.098018 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 65.4%, Avg loss: 1.919036 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 69.5%, Avg loss: 1.640180 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Tranning the with 50% of the traning data\n",
    "def train(dataloader, model, loss_fn, optimizer, device):\n",
    "    model.train()\n",
    "    for X, y in dataloader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "\n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    return test_loss, correct\n",
    "\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(5):\n",
    "    print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
    "    train(initial_dataloader, model, loss_fn, optimizer, device)\n",
    "    test(testloader, model, loss_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e1d0769d-f4c6-40bb-b53d-ccf2760d9974",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_online(OnX, Ony, initial_dataloader, model, loss_fn, optimizer, device):\n",
    "    # storing intial weights\n",
    "    weights_initial = {name: parameter.clone() for name, parameter in model.named_parameters()}\n",
    "    \n",
    "    # traning with 5 batches of initial data\n",
    "    i = 0\n",
    "    for OfX, Ofy in initial_dataloader:\n",
    "        if(i == 5):\n",
    "            break\n",
    "        OfX, Ofy = OfX.to(device), Ofy.to(device)\n",
    "        conX, cony = torch.cat((OnX, OfX)), torch.cat((Ony, Ofy))\n",
    "        output = model(conX)\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_fn(output, cony)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        i += 1\n",
    "        \n",
    "    # modifying the weights with with weigth avarage to avoid catastrophic forgetting\n",
    "    alpha = 0.7\n",
    "    weights_new = {name: parameter.clone() for name, parameter in model.named_parameters()}\n",
    "    for key in model.state_dict():\n",
    "        model.state_dict()[key].data.copy_(alpha * weights_initial[key] + (1 - alpha) * weights_new[key])\n",
    "        \n",
    "\n",
    "## Return accuaracy and loss for the old dataset and new batches\n",
    "def test_online(dataloader, new_batches, model, loss_fn, device, batch_size=32):\n",
    "    size = len(dataloader.dataset) + len(new_batches) * batch_size\n",
    "    num_batches = len(dataloader) + len(new_batches)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in itertools.chain(dataloader, new_batches):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"OldDataset and new batches Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    return test_loss, correct\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4955a8fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Simulating the online learning with new batches:  0\n",
      "Test Error: \n",
      " Accuracy: 69.8%, Avg loss: 1.637130 \n",
      "\n",
      "OldDataset and new batches Error: \n",
      " Accuracy: 68.9%, Avg loss: 1.644118 \n",
      "\n",
      "\n",
      "Simulating the online learning with new batches:  1\n",
      "Test Error: \n",
      " Accuracy: 69.8%, Avg loss: 1.636620 \n",
      "\n",
      "OldDataset and new batches Error: \n",
      " Accuracy: 68.9%, Avg loss: 1.643776 \n",
      "\n",
      "\n",
      "Simulating the online learning with new batches:  2\n"
     ]
    }
   ],
   "source": [
    "New_batches = []\n",
    "j = 0\n",
    "for X, y in online_dataloader:\n",
    "    if(j == 2):\n",
    "        break\n",
    "    \n",
    "    print(\"\\nSimulating the online learning with new batches: \", j + 1)\n",
    "    X, y = X.to(device), y.to(device)\n",
    "    train_online(X, y, initial_dataloader, model, loss_fn, optimizer, device)\n",
    "\n",
    "    # reporting the accuracy of the model on the test data\n",
    "    test(testloader, model, loss_fn)\n",
    "\n",
    "    # reporting the accuracy of the model on the train_data and new batches\n",
    "    New_batches.append((X, y))\n",
    "    test_online(initial_dataloader, New_batches, model, loss_fn, device)\n",
    "    j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97af23b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "round",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "549ea3a47934970ebf23e2f3e70b5e234dc1875edca41c0c27a91da9946e5ece"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
